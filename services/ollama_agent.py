#Imports required libraries for HTTP requests
import requests# Library for making API calls 
from typing import List, Dict, Any, Optional

# Ollama API endpoint (default local installation)
OLLAMA_API_BASE = "http://localhost:11434"
MODEL_NAME = "photography-assistant"  #The models name on the system

class OllamaAgent:
    #agent class for interacting with locally hosted Ollama 
    #handles chat requests, streaming responses and model availbility checks
    def __init__(self, base_url: str = OLLAMA_API_BASE, model: str = MODEL_NAME):
        self.base_url = base_url.rstrip("/")#URL of the Ollama api server
        self.model = model #name of the model were using
        #intialise agent with connection
        

    #send a chat request to Ollama and get a response
    #messages has list of message with role and content keys
    #stream is whether response should be streamed   
    #Returns dictionary containing success(if it worked), message(the response) and error(Error message)
    def chat(self, messages: List[Dict[str, str]], stream: bool = False) -> Dict[str, Any]:

        try:
            #construct the full api endpoint URL for the bots chat
            url = f"{self.base_url}/api/chat"
            #data were sending to OLLAMA
            payload = {
                "model": self.model, #what model to use
                "messages": messages, #conversation history
                "stream": stream #whether to give response
            }
            #post request to OLLama API with 60 second timeout
            response = requests.post(url, json=payload, timeout=60)
            response.raise_for_status()
            
            if stream:#handle streaming vs complete response 
                # Return the response object for streaming
                return {"success": True, "response": response}
            else:
                # Parse complete response
                data = response.json()
                #extract the assistants message from nested JSON
                assistant_message = data.get("message", {}).get("content", "")
                #return structured success response
                return {
                    "success": True,
                    "message": assistant_message,
                    "model": data.get("model"),
                    "done": data.get("done", False)
                }
               #handles specfic timeout error from taking to long 
        except requests.exceptions.Timeout:
            return {
                "success": False,
                "error": "Request timed out. The model may be processing a complex query."
            }
        #handles connection errors 
        except requests.exceptions.ConnectionError:
            return {
                "success": False,
                "error": "Cannot connect to Ollama. Make sure Ollama is running (try: ollama serve)"
            }
        #handles other http erros
        except requests.exceptions.RequestException as e:
            return {
                "success": False,
                "error": f"API request failed: {str(e)}"
            }
        #catch all for any errors
        except Exception as e:
            return {
                "success": False,
                "error": f"Unexpected error: {str(e)}"
            }
    #stream responses from Ollama in real time chunks 
    #Args: messages are lists of conversation messages
    #Yields: string chunks of the response as they are generated by ollama
    def chat_stream(self, messages: List[Dict[str, str]]):
        try:
            #construct API endpoint for chat
            url = f"{self.base_url}/api/chat"
            #prepare payload with stream enabled
            payload = {
                "model": self.model,#model being used
                "messages": messages,#conversation history
                "stream": True #enables streaming mode
            }
            #Make post request with streaming enabled
            #Stream= true tells requests to download response in chunks
            response = requests.post(url, json=payload, stream=True, timeout=60)
            #checks for HTTP error
            response.raise_for_status()
            
            #Iterate over response line by line as they come
            for line in response.iter_lines():
                if line:#skip empty lines
                    import json#import json for parsing each chunk
                    chunk = json.loads(line)#each line is a JSON object with part of response
                    if "message" in chunk:#extract message content if present
                        content = chunk["message"].get("content", "")
                        if content:#non empty content to caller
                            yield content#generator pattern sends chunk back
            #if any error occurs during streaming error message                
        except Exception as e:
            yield f"\n\n[Error: {str(e)}]"
    
    def is_model_available(self) -> bool:
        #check to see if model is installed and available
        #return true if model exists if not false
        try:
            #construct URL for listing models
            url = f"{self.base_url}/api/tags"
            #GET request to retrieve list of installed models
            response = requests.get(url, timeout=5)
            #raise exception if request fails
            response.raise_for_status()
            #Extract list of models from json response
            models = response.json().get("models", [])
            return any(m.get("name", "").startswith(self.model) for m in models)
        #if any error occurs assume model is not available    
        except Exception:
            return False
    
    def get_model_info(self) -> Optional[Dict[str, Any]]:
        #get information about the model including model size parameter and system prompt
        #request dictionary with model details or none if fails
        try:
            url = f"{self.base_url}/api/show"#construct url for showing model details
            payload = {"name": self.model}#payload specfies which model to get info about
            response = requests.post(url, json=payload, timeout=10)#post request to get model information
            response.raise_for_status()#check for HTTP errors
            return response.json()#return parsed JSON response with model details
        except Exception:#return none if any error occurs
            return None


# Helper function for simple one off questions
def ask_photography_question(question: str) -> str:
    #function for asking single photography question
    #creates agent sends question and returns answer as string
    #ARGs: question users photography question as string
    #Returns: Assistant answer or error message
    agent = OllamaAgent()#create new agent
    messages = [{"role": "user", "content": question}]#format question as a message list
    result = agent.chat(messages)#send question to model and get response
    
    if result["success"]:#check if result was succesful
        return result["message"]#return the assistant's message content 
    else:#return formatted error message
        return f"Error: {result['error']}"